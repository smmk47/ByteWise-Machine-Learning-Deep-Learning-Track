{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39931096-e815-4e7d-968f-4c62ea17e33a",
   "metadata": {},
   "source": [
    "# Part 1: Understanding RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5b5cca-d278-441f-9e11-b1473f5395c3",
   "metadata": {},
   "source": [
    "# Understanding RNNs\n",
    "\n",
    "## What are Recurrent Neural Networks (RNNs)?\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a type of neural network designed for processing sequential data. Unlike traditional feedforward neural networks, where the flow of information is unidirectional from input to output, RNNs have connections that form cycles. This structure allows them to maintain a form of memory by passing information from one step of the sequence to the next.\n",
    "\n",
    "### How do RNNs Differ from Traditional Feedforward Neural Networks?\n",
    "\n",
    "1. **Memory**: RNNs have an internal state or memory that captures information about previous elements in the sequence, which is updated at each time step. This is in contrast to feedforward neural networks, where information only flows in one direction without any feedback loops.\n",
    "\n",
    "2. **Sequence Handling**: RNNs are specifically designed to handle sequences of varying lengths and are effective for tasks such as time series prediction, natural language processing, and sequence generation. Feedforward networks are typically used for tasks where inputs and outputs are fixed in size.\n",
    "\n",
    "## Working of RNNs\n",
    "\n",
    "An RNN processes input sequences one element at a time while maintaining a hidden state vector. The steps involved in processing a sequence are:\n",
    "\n",
    "1. **Input**: At each time step, an input vector is fed into the RNN.\n",
    "2. **Hidden State Update**: The hidden state vector is updated based on the input and the previous hidden state. This update is performed using a combination of the current input and the previous hidden state.\n",
    "3. **Output**: The updated hidden state can be used to produce an output or be passed to the next time step.\n",
    "\n",
    "Mathematically, the hidden state at time step `t` is updated as follows:\n",
    "\n",
    "$$ h_t = f(W_h h_{t-1} + W_x x_t + b) $$\n",
    "\n",
    "where:\n",
    "- \\( h_t \\) is the hidden state at time step \\( t \\).\n",
    "- \\( W_h \\) is the weight matrix for the hidden state.\n",
    "- \\( W_x \\) is the weight matrix for the input.\n",
    "- \\( x_t \\) is the input vector at time step \\( t \\).\n",
    "- \\( b \\) is the bias term.\n",
    "- \\( f \\) is an activation function (e.g., tanh or ReLU).\n",
    "\n",
    "## Stacking RNN Layers and Bi-directional Architecture\n",
    "\n",
    "### Advantages and Potential Drawbacks of Stacking RNN Layers\n",
    "\n",
    "**Advantages:**\n",
    "- **Increased Capacity**: Stacking multiple RNN layers increases the capacity of the model to learn complex patterns and representations from the data.\n",
    "- **Hierarchical Feature Learning**: Each layer can learn different levels of abstractions, with lower layers capturing simple features and higher layers capturing more complex patterns.\n",
    "\n",
    "**Potential Drawbacks:**\n",
    "- **Vanishing/Exploding Gradients**: Deeper RNNs can suffer from vanishing or exploding gradient problems, making training difficult.\n",
    "- **Computational Complexity**: Stacking layers increases the computational resources required for training and inference.\n",
    "\n",
    "### Bi-directional RNNs\n",
    "\n",
    "**Definition:**\n",
    "Bi-directional RNNs (BRNNs) consist of two separate RNNs that process the sequence in both forward and backward directions. This allows the network to capture context from both past and future elements in the sequence.\n",
    "\n",
    "**Enhancements:**\n",
    "- **Improved Context Understanding**: By considering both past and future context, BRNNs can improve performance on tasks where understanding the full sequence is crucial.\n",
    "- **Better Handling of Dependencies**: Useful for tasks like machine translation and speech recognition where future context improves prediction accuracy.\n",
    "\n",
    "## Hybrid Architecture\n",
    "\n",
    "**Definition:**\n",
    "A hybrid architecture in sequence modeling refers to the combination of RNNs with other types of neural networks to leverage their complementary strengths.\n",
    "\n",
    "**Examples:**\n",
    "1. **RNN with Convolutional Neural Networks (CNNs)**: Combining RNNs with CNNs can enhance performance by using CNNs to extract features from input sequences (e.g., text or images) before feeding them into an RNN.\n",
    "2. **RNN with Attention Mechanisms**: Attention mechanisms help RNNs focus on relevant parts of the input sequence, improving performance on tasks like machine translation and text summarization.\n",
    "\n",
    "## Types of RNN Models\n",
    "\n",
    "1. **Vanilla RNNs**\n",
    "   - **Structure**: Basic form of RNN with simple feedback connections.\n",
    "   - **Differences**: Limited in capturing long-term dependencies due to vanishing gradient problems.\n",
    "\n",
    "2. **Long Short-Term Memory (LSTM) Networks**\n",
    "   - **Structure**: Includes memory cells and gating mechanisms (input, forget, and output gates) to manage long-term dependencies.\n",
    "   - **Differences**: Designed to overcome the vanishing gradient problem and capture long-range dependencies better than vanilla RNNs.\n",
    "\n",
    "3. **Gated Recurrent Units (GRUs)**\n",
    "   - **Structure**: Similar to LSTMs but with a simplified gating mechanism (update and reset gates).\n",
    "   - **Differences**: Generally faster to train than LSTMs with comparable performance.\n",
    "\n",
    "4. **Bidirectional RNNs (BRNNs)**\n",
    "   - **Structure**: Processes sequences in both forward and backward directions using two separate RNNs.\n",
    "   - **Differences**: Captures context from both directions, improving performance on tasks where future information is important.\n",
    "\n",
    "5. **Attention-based RNNs**\n",
    "   - **Structure**: Incorporates attention mechanisms to focus on different parts of the input sequence.\n",
    "   - **Differences**: Enhances the RNN's ability to handle long sequences and improves interpretability of the model's decisions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcd1c80-9823-4a4f-943b-08dc06ab3543",
   "metadata": {},
   "source": [
    "# Part :2 Implementing RNN Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7bf0f4-9648-4c5f-97bd-ad141f49113d",
   "metadata": {},
   "source": [
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e88ba2a-4384-477c-8791-5c8b44baaf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Basic RNN...\n",
      "Epoch 1/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 110ms/step - accuracy: 0.6740 - loss: 0.5830 - val_accuracy: 0.8304 - val_loss: 0.4055\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 110ms/step - accuracy: 0.8447 - loss: 0.3627 - val_accuracy: 0.7894 - val_loss: 0.4490\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 109ms/step - accuracy: 0.8575 - loss: 0.3444 - val_accuracy: 0.7860 - val_loss: 0.4792\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 107ms/step - accuracy: 0.8802 - loss: 0.2991 - val_accuracy: 0.8366 - val_loss: 0.4120\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 142ms/step - accuracy: 0.9147 - loss: 0.2257 - val_accuracy: 0.8210 - val_loss: 0.4738\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 113ms/step - accuracy: 0.9509 - loss: 0.1376 - val_accuracy: 0.8060 - val_loss: 0.5862\n",
      "Epoch 7/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 110ms/step - accuracy: 0.9506 - loss: 0.1389 - val_accuracy: 0.7890 - val_loss: 0.6216\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 109ms/step - accuracy: 0.9829 - loss: 0.0604 - val_accuracy: 0.8166 - val_loss: 0.6969\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 110ms/step - accuracy: 0.9674 - loss: 0.0944 - val_accuracy: 0.7978 - val_loss: 0.6967\n",
      "Epoch 10/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 111ms/step - accuracy: 0.9905 - loss: 0.0397 - val_accuracy: 0.8092 - val_loss: 0.7534\n",
      "Basic RNN Model: Loss = 0.7763176560401917, Accuracy = 80.33%\n",
      "\n",
      "Training Stacked RNN...\n",
      "Epoch 1/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 216ms/step - accuracy: 0.6160 - loss: 0.6233 - val_accuracy: 0.8052 - val_loss: 0.4280\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 215ms/step - accuracy: 0.8535 - loss: 0.3403 - val_accuracy: 0.8326 - val_loss: 0.4177\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 213ms/step - accuracy: 0.9008 - loss: 0.2505 - val_accuracy: 0.8232 - val_loss: 0.4468\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 214ms/step - accuracy: 0.9513 - loss: 0.1399 - val_accuracy: 0.8168 - val_loss: 0.5486\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 215ms/step - accuracy: 0.9789 - loss: 0.0635 - val_accuracy: 0.8220 - val_loss: 0.6920\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 213ms/step - accuracy: 0.9870 - loss: 0.0416 - val_accuracy: 0.7768 - val_loss: 0.7963\n",
      "Epoch 7/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 212ms/step - accuracy: 0.9829 - loss: 0.0499 - val_accuracy: 0.7544 - val_loss: 0.8930\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 212ms/step - accuracy: 0.9897 - loss: 0.0336 - val_accuracy: 0.7878 - val_loss: 0.8125\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 212ms/step - accuracy: 0.9895 - loss: 0.0316 - val_accuracy: 0.7386 - val_loss: 1.0930\n",
      "Epoch 10/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 217ms/step - accuracy: 0.9957 - loss: 0.0157 - val_accuracy: 0.7454 - val_loss: 0.9834\n",
      "Stacked RNN Model: Loss = 0.9857452511787415, Accuracy = 74.89%\n",
      "\n",
      "Training Bidirectional RNN...\n",
      "Epoch 1/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 195ms/step - accuracy: 0.5622 - loss: 0.6687 - val_accuracy: 0.7306 - val_loss: 0.5367\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 198ms/step - accuracy: 0.8048 - loss: 0.4399 - val_accuracy: 0.8274 - val_loss: 0.4108\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 189ms/step - accuracy: 0.8743 - loss: 0.3012 - val_accuracy: 0.8230 - val_loss: 0.4457\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 186ms/step - accuracy: 0.9336 - loss: 0.1851 - val_accuracy: 0.6804 - val_loss: 0.7104\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 189ms/step - accuracy: 0.9624 - loss: 0.1190 - val_accuracy: 0.7654 - val_loss: 0.7002\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m414s\u001b[0m 664ms/step - accuracy: 0.9815 - loss: 0.0623 - val_accuracy: 0.7688 - val_loss: 0.7488\n",
      "Epoch 7/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2400s\u001b[0m 4s/step - accuracy: 0.9904 - loss: 0.0339 - val_accuracy: 0.7578 - val_loss: 0.8776\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 211ms/step - accuracy: 0.9967 - loss: 0.0159 - val_accuracy: 0.7584 - val_loss: 0.9540\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 190ms/step - accuracy: 0.9977 - loss: 0.0134 - val_accuracy: 0.7998 - val_loss: 0.9911\n",
      "Epoch 10/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 190ms/step - accuracy: 0.9813 - loss: 0.0523 - val_accuracy: 0.7636 - val_loss: 0.9698\n",
      "Bidirectional RNN Model: Loss = 0.9942663311958313, Accuracy = 76.02%\n",
      "\n",
      "Training Hybrid RNN with Conv1D...\n",
      "Epoch 1/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 26ms/step - accuracy: 0.6586 - loss: 0.5975 - val_accuracy: 0.8488 - val_loss: 0.3557\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 25ms/step - accuracy: 0.8800 - loss: 0.2909 - val_accuracy: 0.8790 - val_loss: 0.2976\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 25ms/step - accuracy: 0.9375 - loss: 0.1794 - val_accuracy: 0.8782 - val_loss: 0.3024\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 24ms/step - accuracy: 0.9641 - loss: 0.1193 - val_accuracy: 0.8832 - val_loss: 0.3113\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 25ms/step - accuracy: 0.9840 - loss: 0.0700 - val_accuracy: 0.8816 - val_loss: 0.3422\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 25ms/step - accuracy: 0.9940 - loss: 0.0409 - val_accuracy: 0.8800 - val_loss: 0.3812\n",
      "Epoch 7/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 25ms/step - accuracy: 0.9983 - loss: 0.0210 - val_accuracy: 0.8782 - val_loss: 0.4171\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 24ms/step - accuracy: 0.9994 - loss: 0.0117 - val_accuracy: 0.8792 - val_loss: 0.4678\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 25ms/step - accuracy: 0.9998 - loss: 0.0054 - val_accuracy: 0.8750 - val_loss: 0.5013\n",
      "Epoch 10/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 25ms/step - accuracy: 0.9998 - loss: 0.0054 - val_accuracy: 0.8756 - val_loss: 0.5334\n",
      "Hybrid RNN with Conv1D Model: Loss = 0.5289185643196106, Accuracy = 86.98%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Bidirectional, Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "# Parameters\n",
    "max_features = 5000  # Number of words to consider as features\n",
    "maxlen = 500         # Cuts off texts after this number of words\n",
    "batch_size = 32      # Size of the batches of data\n",
    "\n",
    "# Load data\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "# Basic RNN Model\n",
    "model_basic = Sequential([\n",
    "    Embedding(max_features, 32),\n",
    "    SimpleRNN(32),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model_basic.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Stacked RNN Model\n",
    "model_stacked = Sequential([\n",
    "    Embedding(max_features, 32),\n",
    "    SimpleRNN(32, return_sequences=True),\n",
    "    SimpleRNN(32),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model_stacked.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Bidirectional RNN Model\n",
    "model_bidirectional = Sequential([\n",
    "    Embedding(max_features, 32),\n",
    "    Bidirectional(SimpleRNN(32)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model_bidirectional.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Hybrid Model: RNN with Convolutional Layer\n",
    "model_hybrid = Sequential([\n",
    "    Embedding(max_features, 32),\n",
    "    Conv1D(32, 7, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model_hybrid.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train and evaluate all models\n",
    "models = [model_basic, model_stacked, model_bidirectional, model_hybrid]\n",
    "model_names = [\"Basic RNN\", \"Stacked RNN\", \"Bidirectional RNN\", \"Hybrid RNN with Conv1D\"]\n",
    "for model, name in zip(models, model_names):\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=10, validation_split=0.2)\n",
    "    scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"{name} Model: Loss = {scores[0]}, Accuracy = {scores[1]*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acba1c6c-a8e1-48c7-917e-afe4fd8c16f4",
   "metadata": {},
   "source": [
    "### Explanation of Training Results for Different RNN Architectures\n",
    "\n",
    "The code you provided outlines the implementation and training of four different Recurrent Neural Network (RNN) models on the IMDb movie review dataset, aiming to classify reviews as positive or negative. Here's a detailed explanation of each model and their performance:\n",
    "\n",
    "#### 1. **Basic RNN Model**\n",
    "   - **Architecture**: This model consists of an Embedding layer, a Simple RNN layer, and a Dense layer with a sigmoid activation function. \n",
    "   - **Performance**: The model reached an accuracy of about 80.33% on the test set. The training process shows that while accuracy improved over epochs, the validation loss began increasing after a few epochs, suggesting some overfitting.\n",
    "\n",
    "#### 2. **Stacked RNN Model**\n",
    "   - **Architecture**: This model extends the basic RNN by stacking two Simple RNN layers, which can potentially capture more complex patterns in the sequence data.\n",
    "   - **Performance**: The accuracy on the test set was approximately 74.89%. Similar to the basic RNN, this model showed signs of overfitting as indicated by increasing validation losses despite improving training losses. This model's performance was slightly lower than the basic RNN, which might suggest that the added complexity did not generalize well.\n",
    "\n",
    "#### 3. **Bidirectional RNN Model**\n",
    "   - **Architecture**: This configuration uses a Bidirectional wrapper around a Simple RNN layer, allowing the network to learn from both past (forward) and future (backward) dependencies.\n",
    "   - **Performance**: It achieved about 76.02% accuracy on the test data. Despite the theoretical advantage of capturing information from both directions, the performance was not significantly better than the basic model and still showed substantial overfitting.\n",
    "\n",
    "#### 4. **Hybrid RNN with Conv1D Model**\n",
    "   - **Architecture**: This hybrid model includes a convolutional layer before the RNN structure. Conv1D layers can help capture local dependencies in the sequence data, potentially providing more relevant features for the RNN layer.\n",
    "   - **Performance**: This model performed the best among the four, with an accuracy of 86.98% on the test dataset. It also exhibited better control over overfitting compared to the other models, as shown by more stable validation losses.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- **Performance Ranking**: The Hybrid RNN with Conv1D model outperformed the other architectures by a significant margin, suggesting that combining convolutional layers with RNN layers can be effective for sequence data processing tasks like sentiment analysis.\n",
    "  \n",
    "- **Overfitting Issue**: Except for the hybrid model, all models displayed tendencies of overfitting, where the validation loss increased as the training progressed. This suggests that while RNNs are powerful for sequence modeling, they are also prone to memorizing training data, especially with longer training or more complex architectures.\n",
    "\n",
    "- **General Recommendation**: Incorporating convolutional layers as feature extractors before RNN layers can mitigate some of the overfitting issues and improve model performance on tasks involving sequential input data. This approach appears to offer a more balanced feature extraction mechanism, potentially making it more robust to variations in test data.\n",
    "\n",
    "These insights could guide the choice and design of neural network architectures for similar natural language processing tasks, especially where the balance between model complexity and generalization is critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf1ef1b-ebf4-4ea6-a5d0-7952ca4afabd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69ab00d-bb3b-4dc1-a406-e7c3d01b2fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1701974b-d023-4498-a9fa-98aee17f2837",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433a87c5-b037-42e1-af71-5b4006c45ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a44f1f8-516e-4a94-b857-c07feaf4a15d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92d8028-b2c9-40e4-b2e6-7755118c146c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c5e03a-afe0-4a59-b20e-1124a4f5b8bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5829461c-8de9-4bd5-9f9d-77ab97f4eab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1525d0-18cd-46ea-96a4-70df8fb6b2a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4b7b49-1886-4d1e-a971-881d266e3939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939e7dde-53c4-44fa-8d67-ade51f7e1d90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d04803-298d-4c6e-9db4-1052b6317339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250de0fc-b32b-4a82-9a68-abaec531acf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8133d4-f49a-41cd-aca2-efb8c8958c58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfcbbe8-ddd5-4c61-9dc0-fe18e5f12c34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cf12fb-0e05-4b77-bde6-53759f339349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9bdf81-c464-45d8-959c-fea36cdd08de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d8a7d7-458a-4b56-a1b5-e83f870b1b1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b3659f-077d-4f5d-8e38-e5814c69d4d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78b632b-68ba-4005-b155-ba6c6527cf18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7953628-b777-43d8-8e8b-b0ac9b585c36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f6caa1-3ea3-4a31-a28d-d65ceacabab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994302b1-7f2d-40cf-8d41-4148821f1a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895934e1-0e88-4025-a5b0-1d92e5d674f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a0c8b8-bd77-4c3d-a30d-ac118789d460",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
